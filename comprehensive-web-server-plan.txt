User: ok - the survelliance cameras - I have a couple of requirements I am looking at. 1 when motion is captured, images are captured or data is saved to the card, we should be able to view that data remotely in one single location regardless of how many cameras are on the network. would also be nice if we could view them in some sort of web ui. Can you propose any ideas?

GitHub Copilot: Research surveillance camera system architecture

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/README.md#1-1), lines 1 to 200

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/src/main.cpp#1-1), lines 1 to 300

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/include/device_config.h)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/include/camera_config.h)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/activeContext.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/progress.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/temperature-sensor/src/main.cpp#1-1), lines 1 to 150

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/solar-monitor/src/main.cpp#1-1), lines 1 to 150

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/temperature-sensor/platformio.ini)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/solar-monitor/platformio.ini)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/platformio.ini)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/productContext.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/solar-monitor/README.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/temperature-sensor)

Searched for regex `influxdb|InfluxDB|INFLUX`, 20 results

Searched for regex `mqtt broker|MQTT_SERVER|MQTT_PORT`, 20 results

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/README.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/src/main.cpp#300-300), lines 300 to 600

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/src/main.cpp#600-600), lines 600 to 1000

Perfect! Now I have comprehensive context. Let me create a detailed plan.

## Plan: Centralized Surveillance Dashboard with Multi-Camera View

**TL;DR**: Create a centralized web dashboard that aggregates motion events and image captures from all surveillance cameras, backed by MQTT events and stored JPEG files. Leverage existing InfluxDB/Grafana infrastructure for metrics, add a custom web UI (Node.js/Express + Vue/React) for image gallery, and implement automatic image sync to a shared backend storage.

### Steps

1. **Design multi-camera data aggregation layer**: Create a Node.js/Express backend service running on Raspberry Pi that subscribes to all camera MQTT topics (`surveillance/[device-name]/motion`, `/events`), aggregates motion events with timestamps, and maintains a unified event log in a database (PostgreSQL or extend InfluxDB).

2. **Implement centralized image storage & indexing**: Set up a backend image directory/S3-compatible storage and create a metadata table indexed by device, timestamp, and motion event. Have each camera either send base64-encoded JPEG to a dedicated backend API endpoint on motion, or sync captured JPEG files via SFTP/SCP to centralized storage.

3. **Build responsive web UI dashboard**: Create a gallery-style interface showing:
   - Timeline view of all motion events across all cameras (sorted by time)
   - Gallery grid with camera thumbnails and event thumbnails
   - Filters by device, date range, and motion event type
   - Individual modal/lightbox to view full JPEG + metadata (device, timestamp, motion score)

4. **Integrate with existing MQTT → InfluxDB → Grafana pipeline**: Ensure motion event metadata (device, timestamp, motion_detected, pixel_change_count) flows to InfluxDB via Telegraf for historical analytics. Add Grafana dashboard showing motion heatmaps by camera and time-of-day patterns.

5. **Add remote access capability**: Use a reverse proxy (Nginx) with basic auth or implement VPN tunneling for secure remote viewing (e.g., Cloudflare Tunnel, Tailscale, or self-hosted WireGuard).

6. **Implement automatic image retention & cleanup**: Add cron job on backend to archive old images and enforce a retention window (e.g., 7-14 days), preventing SD card from filling on long-running deployments.

### Further Considerations

1. **Storage & Bandwidth Trade-offs**:
   - **Option A** (Local SFTP sync): Cameras push JPEG files to Raspberry Pi over LAN—minimal bandwidth, requires SFTP setup on backend.
   - **Option B** (MQTT Base64): Cameras publish base64 JPEG to MQTT topic—simpler code, higher MQTT broker load, slower for large images.
   - **Option C** (Hybrid): Send motion event + low-res thumbnail via MQTT immediately; store full JPEG on SD card; sync full images on scheduled interval (e.g., 1x/minute).

2. **Tech Stack Decision**:
   - **Backend**: Existing Telegraf/InfluxDB or add lightweight Node.js server?
   - **UI Framework**: Pre-built dashboard (Home Assistant, Node-RED, Grafana plugin) vs. custom Vue/React SPA?
   - **Database**: Reuse InfluxDB with custom metadata queries, or add PostgreSQL for rich indexing?

3. **Hardware & Timeline Constraints**:
   - How many cameras are planned? (impacts storage: 20 cameras × 100 events/day × 25KB = ~50GB/month)
   - Internet/LAN bandwidth available for sync?
   - Is this for local-network only or remote internet access?

User: probably need a postgresql database. we can house all this on docker containers on raspberrypi. Option A is simplest for storage - hardware cameras - plan for 5 at least. I dont see a need or anymore than that. local network although I could setup a cloudflare tunnel and nginx to view remotely

GitHub Copilot: Searched codebase for "docker compose raspberry pi infrastructure services MQTT broker InfluxDB", 19 results

Searched codebase for "deployment architecture backend services setup infrastructure", 17 results

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor)

Great! Now I have the full context. Let me refine the plan based on your specific constraints:

## Plan: Multi-Camera Surveillance Dashboard with PostgreSQL & Docker

**TL;DR**: Deploy a centralized backend (Node.js/Express API + PostgreSQL database) and web UI dashboard as Docker containers on Raspberry Pi. Cameras sync JPEG captures via SFTP to shared storage. Dashboard aggregates motion events from MQTT, displays gallery of all captures indexed by device/timestamp, and includes optional Cloudflare Tunnel + Nginx for remote access.

### Steps

1. **Create Docker Compose stack on Raspberry Pi**: Define services for PostgreSQL (database), Node.js backend API (Express), web UI (React/Vue), Nginx (reverse proxy + optional remote tunnel), alongside existing Mosquitto MQTT broker, InfluxDB, and Grafana.

2. **Design PostgreSQL schema for motion events & captures**: Create tables for `cameras` (device metadata), `motion_events` (timestamp, device, pixel_change_score, image_path), and `image_metadata` (filename, device, timestamp, size, motion_triggered). Index on device_id + timestamp for fast queries.

3. **Implement SFTP server on backend**: Set up SFTP daemon to receive JPEG files from all cameras. Cameras sync captures to `/mnt/surveillance/captures/{device-name}/` directory with graceful overwrite/cleanup. Backend validates file integrity on arrival and updates PostgreSQL metadata.

4. **Build Node.js backend API**: Create endpoints for:
   - `GET /api/events?device=&start=&end=` - paginated motion events + metadata
   - `GET /api/gallery/{device}` - thumbnail gallery with motion filters
   - `POST /api/events/sync` - receive motion event payloads from MQTT (via separate service)
   - `DELETE /api/captures?older_than=days` - auto-cleanup on schedule

5. **Build responsive web UI dashboard**: Create single-page app showing timeline of all motion events across all 5 cameras, filterable by device/date, with lightbox viewer for full JPEG captures + metadata. Reuse existing Mosquitto/InfluxDB/Grafana infrastructure for metrics layer.

6. **Add optional remote access (Cloudflare Tunnel + Nginx)**: Configure Nginx reverse proxy with basic auth, tunnel to `camera-dashboard.your-domain.com` via Cloudflare Tunnel for secure remote viewing without port forwarding or VPN.

### Further Considerations

1. **SFTP Sync Strategy** (Option A details):
   - Cameras push JPEG on motion detection via SFTP (SSH protocol, TCP 22, no port forwarding needed)
   - Minimal bandwidth: only new JPEGs; existing files skipped
   - Backend creates symlinks or database entries for gallery indexing
   - **Trade-off**: Requires SSH key setup on cameras vs. simpler MQTT base64 approach

2. **Storage & Retention**:
   - 5 cameras × 100 events/day × 25KB JPEG = ~12.5GB/month (manageable for Pi's SD card)
   - Recommend cron job to archive/delete captures >7-14 days old (PostgreSQL tracks `created_at`)
   - Symlink old files to external USB drive if needed

3. **Docker Compose Stack Definition**:
   - Should this be a **new `docker-compose.yml`** in project root, or **extend existing** Mosquitto/InfluxDB stack?
   - Recommendation: New `docker-compose.surveillance.yml` for modularity; can run standalone or alongside other services

User: raspberrypi has the telegraf, promteheus, influxdb, mqtt  stack, etc.. raspberrypi2 has capacity and has some containers on it - all raspberrypi3 has no containers

GitHub Copilot: ## Plan: Central Surveillance Stack on Pi Fleet

All cameras (≥5) push motion JPEGs via SFTP to a centralized Node.js/Express backend with PostgreSQL metadata, exposed through a web UI. Use existing MQTT/Telegraf/Prometheus/InfluxDB stack on Raspberry Pi; add new containers on Raspberry Pi 2; keep Raspberry Pi 3 idle/standby.

### Steps
1. Deploy new Docker stack on Raspberry Pi 2: PostgreSQL, Node.js/Express API, web UI (React/Vue), SFTP container for camera uploads, Nginx reverse proxy; mount `/mnt/surveillance/captures`.
2. Wire MQTT ingestion: Raspberry Pi (existing broker) → small MQTT consumer container (could run on Pi 2) that listens to `surveillance/+/motion` and posts events to API/PostgreSQL.
3. Define PostgreSQL schema: `cameras`, `motion_events` (device_id, ts, score, image_path), `images` (file metadata, ts, device_id), with indexes on device_id+ts.
4. Configure cameras for SFTP push: per-camera SSH key, target path `/captures/{device}/`, send JPEG on motion, include event metadata in MQTT payload.
5. Build web UI on Pi 2: timeline + gallery across all devices, filters by device/date, lightbox for full image, basic auth via Nginx; serve via `https://camera-dashboard.local`.
6. Add remote access option: Cloudflare Tunnel fronting Nginx on Pi 2; DNS `camera-dashboard.your-domain.com`; rate-limit and basic auth enabled.

### Further Considerations
1. Storage/retention: 5 cams × 100 events/day × 25KB ≈ 12.5GB/month; add cron in API to purge/archive >14 days; optional external SSD on Pi 2.
2. Placement: keep MQTT/Telegraf/InfluxDB/Grafana on Raspberry Pi; run new API/UI/Postgres/SFTP on Raspberry Pi 2; keep Raspberry Pi 3 as cold spare or for future CDN/cache.
3. Sync strategy confirmation: Proceed with Option A (SFTP push) for bandwidth efficiency; fallback to MQTT thumbnail + nightly SFTP full-sync if intermittent Wi-Fi.

User: nginx is running on raspberrypi ( check /etc/hosts for all hosts btw) - can we reuse that - let me know

GitHub Copilot: Research Nginx reuse guidance

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/README.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/reference/PLATFORM_GUIDE.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/reference/CONFIG.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/activeContext.md)

Searched for regex `nginx|reverse proxy|proxy_pass|grafana|influxdb|telegraf|prometheus|mqtt|mosquitto|docker-compose|docker compose|raspberry pi|pi 4|pi4|pi 3|pi3|pi 2|pi2|hosts` (`**/*.{md,sh,ps1,py,txt,json,yml,yaml,ini}`), 200 results

Searched for regex `nginx|reverse proxy|proxy_pass|/etc/hosts|hosts file|dashboard` (`**/*.md`), 16 results

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/productContext.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/decisionLog.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/progress.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/memory-bank/systemPatterns.md)

Searched for regex `docker-compose|compose.yml|docker compose|docker stack|raspberry pi` (`**/*.md`), 8 results

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/README.md)

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/README.md)

Searched for regex `/etc/hosts|hosts file|\.local|your-pi|pi\.|pi-|pi\s` (`**/*.md`), 33 results

Searched for files matching `**/docker-compose*.yml`, no matches

Searched for regex `docker compose|docker-compose|compose`, 100 results

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/README.md#40-40), lines 40 to 90

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/reference/PLATFORM_GUIDE.md#20-20), lines 20 to 60

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/README.md#40-40), lines 40 to 66

## Plan: Reuse Existing Nginx (if present) or Add on Pi 2

You said Nginx is running on the main Raspberry Pi; repo docs don’t show its config. We should confirm its setup, then decide whether to add the new camera dashboard site there or run a separate proxy on Pi 2.

### Steps
1. Verify current Nginx deployment on the main Pi: check if it’s host-installed vs. containerized, list sites (`/etc/nginx/sites-enabled/*`), and note any existing reverse-proxy patterns/ports in use.
2. Decide placement for the camera dashboard: reuse main Pi’s Nginx (add server block + upstream to Pi 2 API/UI), or run Nginx on Pi 2 and front it via the main Pi (or Cloudflare Tunnel).
3. Define hostname and TLS approach: choose `camera-dashboard.<your-domain>` or local host entry; decide on Let’s Encrypt/Cloudflare-origin cert vs. local self-signed; ensure no port conflicts (Grafana 3000, InfluxDB 8086, MQTT 1883).
4. Add Nginx config: new server block routing `/` to web UI and `/api` to Node/Express on Pi 2; enable gzip, caching for static assets, basic auth if desired; include health-check endpoint.
5. Wire remote access if needed: Cloudflare Tunnel from the main Pi to the Nginx site; restrict to chosen hostname and enable auth/rate-limits.

### Further Considerations
1. Where should the dashboard API/UI run? Option A: Pi 2 (isolation, lower risk). Option B: main Pi (fewer hops, but shared resources).
2. Storage path: confirm capture storage mount on Pi 2 (`/mnt/surveillance/captures`) and ensure Nginx has read access if serving images directly.
3. If main Pi Nginx is already terminating TLS, prefer reusing it; otherwise, consider a dedicated Nginx on Pi 2 fronted by the main Pi or Cloudflare Tunnel.

User: there is a samba mount on all pi's - /mnt/nas-backup to a storage device hooked up to the router

GitHub Copilot: ## Plan: Centralized Camera Dashboard Using Existing Nginx + NAS Mount

All cameras push motion JPEGs via SFTP to Pi 2, store under `/mnt/nas-backup/surveillance/captures`, and publish motion events to MQTT. Pi 2 runs API/UI + PostgreSQL; Pi 1’s existing Nginx reverse-proxies a new vhost to Pi 2. Optional Cloudflare Tunnel on Pi 1 for remote access.

### Steps
1. Inventory Pi 1 Nginx: confirm host install vs container, list enabled sites, note free ports/hostnames; reserve a vhost (e.g., `camera-dashboard.local` or `camera-dashboard.your-domain`).
2. Stand up Pi 2 stack via Docker: PostgreSQL, Node/Express API, React/Vue UI, SFTP server; mount `/mnt/nas-backup/surveillance/captures` for shared storage.
3. Wire MQTT → API: lightweight consumer (could be sidecar on Pi 2) subscribes to `surveillance/+/motion`, inserts events into PostgreSQL, links filenames from SFTP uploads.
4. Configure cameras: per-device SSH key for SFTP to Pi 2, upload path `/captures/{device}/`, push JPEG on motion; ensure MQTT payload includes device, timestamp, and filename.
5. Add Nginx vhost on Pi 1: proxy `/` to UI on Pi 2, `/api` to API on Pi 2; enable gzip, basic auth, and caching for static assets; serve images directly from the NAS mount if desired.
6. Remote access option: run Cloudflare Tunnel on Pi 1 pointing to the new vhost; gate with basic auth and rate limits.

### Further Considerations
1. Retention: 5 cams × 100 events/day × 25KB ≈ 12.5GB/month; add 14–30 day purge job and optional archive to deeper NAS path.
2. Bandwidth: SFTP full-resolution to NAS; if Wi‑Fi is spotty, consider sending MQTT thumbnails + periodic SFTP full sync.
3. TLS: If Pi 1 Nginx already terminates TLS, reuse it; otherwise, add Let’s Encrypt/Cloudflare origin certs on the new vhost.

User: pi's are cabled to the router

GitHub Copilot: Search NAS mount and network topology references

Searched for regex `nas|/mnt/nas-backup|samba|raspberry pi|raspi|pi4|pi 4|pi-zero|pi zero|pi0|raspberry` (`**/*.{md,txt,env,ini,cfg,config,json,yml,yaml}`), 6 results

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/README.md#30-30), lines 30 to 90

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/README.md#200-200), lines 200 to 229

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/README.md#40-40), lines 40 to 66

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/reference/PLATFORM_GUIDE.md#40-40), lines 40 to 90

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/docs/solar-monitor/README.md#440-440), lines 440 to 501

Searched for regex `/mnt|nas-backup|samba|SMB|CIFS|smb://|cifs` (`**/*.{md,txt,env,ini,cfg,config,json,yml,yaml}`), no results

Searched for regex `/mnt|nas-backup|samba|SMB|CIFS|smb://|cifs`, 108 results

Searched for text `NAS` (`**/*.md`), no results

Searched for text `NAS`, 200 results

Searched for regex `Raspberry Pi|Pi 4|Pi4|raspberry` (`**/memory-bank/**/*.md`), no results

Searched for regex `Raspberry Pi|Pi 4|Pi4|raspberry` (`**/memory-bank/**`), no results

Searched for regex `Raspberry Pi|Pi 4|Pi4|raspberry` (`**/docs/architecture/**`), no results

## Plan: Central Dashboard with NAS Storage (Cabled Pis)

Cameras push motion JPEGs via SFTP to Pi 2, stored on the shared NAS mount `/mnt/nas-backup/surveillance/captures`. Pi 2 runs API/UI + PostgreSQL; Pi 1’s existing Nginx reverse-proxies the dashboard. Pis are wired to the router for stable throughput. Pi 3 stays idle/standby.

### Steps
1. **Confirm Pi 1 Nginx and mounts**: Verify Nginx install (host vs container), existing sites, and that `/mnt/nas-backup` is mounted and readable; pick a vhost (e.g., `camera-dashboard.local` or `camera-dashboard.your-domain`).
2. **Deploy Pi 2 stack (Docker)**: PostgreSQL, Node/Express API, React/Vue UI, SFTP server; bind `/mnt/nas-backup/surveillance/captures` into containers for shared storage.
3. **MQTT → API ingestion**: Add a lightweight consumer on Pi 2 subscribed to `surveillance/+/motion`, inserting events into PostgreSQL and linking to uploaded filenames.
4. **Camera config**: Per-camera SSH key to SFTP on Pi 2; upload to `/captures/{device}/`; include device/timestamp/filename in MQTT payload; ensure consistent filenames for indexing.
5. **Nginx vhost on Pi 1**: Proxy `/` to UI on Pi 2, `/api` to API on Pi 2; enable gzip, basic auth, and caching for static assets; optionally serve images directly from the NAS path if permissions allow.
6. **Remote access (optional)**: Cloudflare Tunnel on Pi 1 pointing to the new vhost; enforce basic auth/rate limits; keep MQTT/Intranet ports closed externally.

### Further Considerations
1. **Retention & storage**: 5 cams × 100 events/day × 25KB ≈ 12.5GB/month; set 14–30 day purge and optional archive to deeper NAS path.
2. **Network & bandwidth**: Cabled Pis are good; consider MQTT thumbnails + SFTP full-res fallback if Wi‑Fi to cameras is spotty.
3. **TLS & auth**: If Pi 1 Nginx already terminates TLS, reuse it with the new vhost; otherwise add Let’s Encrypt/Cloudflare origin certs and basic auth.

User: update the planning

GitHub Copilot: Optimizing tool selection...## Updated Plan (cabled Pis, existing Nginx, NAS)

- **Topology**: Pi 1 keeps existing stack (MQTT/Telegraf/InfluxDB/Prometheus/Grafana) and existing Nginx; Pi 2 hosts new camera stack (API/UI/PostgreSQL/SFTP); Pi 3 idle/standby. Pis are wired; storage shared via `/mnt/nas-backup`.
- **Storage**: Mount `/mnt/nas-backup/surveillance/captures` into Pi 2 containers; cameras upload JPEGs there via SFTP. Retention job purges/archives after 14–30 days.
- **Ingestion**: Cameras send motion MQTT events (`surveillance/+/motion`) plus filename; a lightweight consumer on Pi 2 writes PostgreSQL records linking device/timestamp/filename.
- **App stack (Pi 2)**: Docker services—PostgreSQL, Node/Express API, React/Vue UI, SFTP server. Bind NAS path into API/UI (read) and SFTP (write).
- **Nginx (Pi 1)**: Add vhost to proxy `/` to UI on Pi 2 and `/api` to API on Pi 2; enable gzip, basic auth; optionally serve images directly from NAS if perms allow. Reuse existing TLS if present; otherwise add Let’s Encrypt/Cloudflare-origin certs.
- **Remote access (optional)**: Cloudflare Tunnel on Pi 1 to the new vhost; keep MQTT/Influx/Grafana non-public.
- **Bandwidth/resilience**: Full-res via SFTP; if Wi‑Fi to cams is shaky, optionally publish MQTT thumbnails + periodic SFTP full sync.
- **Next checks**: (1) Confirm Pi 1 Nginx mode (host vs container) and available vhost/ports. (2) Confirm NAS mount present on Pi 2 and writable by SFTP container. (3) Choose hostname for the dashboard. (4) Pick retention window (14 vs 30 days).

User: update the entire project plan

GitHub Copilot: ## Updated Project Plan

- **Overall topology**: Pi 1 keeps existing stack (MQTT/Telegraf/InfluxDB/Prometheus/Grafana) and Nginx; Pi 2 runs new camera stack (PostgreSQL, Node/Express API, React/Vue UI, MQTT consumer, SFTP server); Pi 3 is idle/standby. All Pis are cabled. Shared storage is `/mnt/nas-backup/surveillance/captures`.
- **Storage & retention**: Mount NAS on Pi 2 into API/UI/SFTP containers; cameras upload JPEGs via SFTP to `/captures/{device}/`; daily purge/archive job (target 14–30 days).
- **Ingestion path**: Cameras emit MQTT `surveillance/+/motion` with device/timestamp/filename; Pi 2 MQTT consumer writes PostgreSQL records and links to stored files. Optional: MQTT thumbnails if Wi‑Fi to cameras is flaky; full-res via SFTP.
- **App stack (Pi 2, Docker)**: PostgreSQL (metadata), Node/Express API, web UI (React/Vue), SFTP server (per-device SSH keys). Bind NAS read/write as needed; health checks exposed for proxy.
- **Nginx (Pi 1)**: Add vhost for dashboard; proxy `/` to UI on Pi 2, `/api` to API on Pi 2; gzip, basic auth, cache static assets; optionally serve images direct from NAS if permissions allow. Reuse existing TLS or add Let’s Encrypt/Cloudflare-origin certs.
- **Remote access (optional)**: Cloudflare Tunnel on Pi 1 to the new vhost; keep MQTT/Influx/Grafana non-public; enforce auth/rate limits.
- **Security & ops**: SSH key auth for SFTP uploads; least-privileged NAS access for API/UI; monitor disk usage; alert on failed uploads and MQTT consumer lag.
- **Open choices to finalize**:
  1) Hostname for the dashboard (local vs public).  
  2) Retention window (14 vs 30 days) and archive location.  
  3) Whether Nginx on Pi 1 is host-installed or containerized (affects vhost placement).  
  4) Thumbnail-over-MQTT fallback needed or not.

User: where is the webserver camera stuff ?

GitHub Copilot: Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/src/main.cpp#1000-1000), lines 1000 to 1350

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/src/main.cpp#1351-1351), lines 1351 to 1700

Read [](file:///home/aachten/PlatformIO/esp12f_ds18b20_temp_sensor/surveillance/src/main.cpp#1701-1701), lines 1701 to 2050